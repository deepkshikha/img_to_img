{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "JvMRbVLEJlZT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title ðŸ¤— AutoTrain DreamBooth\n",
    "#@markdown In order to use this colab\n",
    "#@markdown - upload images to a folder named `images/`\n",
    "#@markdown - choose a project name if you wish\n",
    "#@markdown - change model if you wish, you can also select sd2/2.1 or sd1.5\n",
    "#@markdown - update prompt and remember it. choose keywords that don't usually appear in dictionaries\n",
    "#@markdown - add huggingface information (token) if you wish to push trained model to huggingface hub\n",
    "#@markdown - update hyperparameters if you wish\n",
    "#@markdown - click `Runtime > Run all` or run each cell individually\n",
    "#@markdown - report issues / feature requests here: https://github.com/huggingface/autotrain-advanced/issues\n",
    "\n",
    "import os\n",
    "!pip install -U autotrain-advanced > install_logs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "A2-_lkBS1WKA",
    "outputId": "b62ac3ad-d078-4479-aaf6-3cf0c267cadc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@markdown ---\n",
    "#@markdown #### Project Config\n",
    "project_name = 'my-dreambooth-project' # @param {type:\"string\"}\n",
    "model_name = 'stabilityai/stable-diffusion-xl-base-1.0' # @param [\"stabilityai/stable-diffusion-xl-base-1.0\", \"runwayml/stable-diffusion-v1-5\", \"stabilityai/stable-diffusion-2-1\", \"stabilityai/stable-diffusion-2-1-base\"]\n",
    "prompt = \"photo a woman of white complexion, with a curvaceous bursty walking on the street, highlight hair, rim lighting, studio lighting, looking at the camera, up close, perfect eyes, bright colour\\\" \\\\\" # @param {type: \"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown #### Push to Hub?\n",
    "#@markdown Use these only if you want to push your trained model to a private repo in your Hugging Face Account\n",
    "#@markdown If you dont use these, the model will be saved in Google Colab and you are required to download it manually.\n",
    "#@markdown Please enter your Hugging Face write token. The trained model will be saved to your Hugging Face account.\n",
    "#@markdown You can find your token here: https://huggingface.co/settings/tokens\n",
    "push_to_hub = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "hf_token = \"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\" #@param {type:\"string\"}\n",
    "hf_username = \"deepshikhasplore\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown #### Hyperparameters\n",
    "learning_rate = 1e-4 # @param {type:\"number\"}\n",
    "num_steps = 500 #@param {type:\"number\"}\n",
    "batch_size = 1 # @param {type:\"slider\", min:1, max:32, step:1}\n",
    "gradient_accumulation = 4 # @param {type:\"slider\", min:1, max:32, step:1}\n",
    "resolution = 1024 # @param {type:\"slider\", min:128, max:1024, step:128}\n",
    "use_8bit_adam = False # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "use_xformers = False # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "mixed_precision = \"fp16\" # @param [\"fp16\", \"bf16\", \"none\"] {type:\"raw\"}\n",
    "train_text_encoder = False # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "disable_gradient_checkpointing = False # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "\n",
    "os.environ[\"PROJECT_NAME\"] = project_name\n",
    "os.environ[\"MODEL_NAME\"] = model_name\n",
    "os.environ[\"PROMPT\"] = prompt\n",
    "os.environ[\"PUSH_TO_HUB\"] = str(push_to_hub)\n",
    "os.environ[\"HF_TOKEN\"] = hf_token\n",
    "os.environ[\"LEARNING_RATE\"] = str(learning_rate)\n",
    "os.environ[\"NUM_STEPS\"] = str(num_steps)\n",
    "os.environ[\"BATCH_SIZE\"] = str(batch_size)\n",
    "os.environ[\"GRADIENT_ACCUMULATION\"] = str(gradient_accumulation)\n",
    "os.environ[\"RESOLUTION\"] = str(resolution)\n",
    "os.environ[\"USE_8BIT_ADAM\"] = str(use_8bit_adam)\n",
    "os.environ[\"USE_XFORMERS\"] = str(use_xformers)\n",
    "os.environ[\"MIXED_PRECISION\"] = str(mixed_precision)\n",
    "os.environ[\"TRAIN_TEXT_ENCODER\"] = str(train_text_encoder)\n",
    "os.environ[\"DISABLE_GRADIENT_CHECKPOINTING\"] = str(disable_gradient_checkpointing)\n",
    "os.environ[\"HF_USERNAME\"] = hf_username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "g3cd_ED_yXXt",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-13 16:37:58\u001b[0m | \u001b[36mautotrain.cli.run_dreambooth\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m388\u001b[0m - \u001b[1mRunning DreamBooth Training\u001b[0m\n",
      "\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2024-05-13 16:37:58\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m174\u001b[0m - \u001b[33m\u001b[1mParameters not supplied by user and set to default: vae_model\u001b[0m\n",
      "\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2024-05-13 16:37:58\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m180\u001b[0m - \u001b[33m\u001b[1mParameters supplied but not used: train, valid_split, inference, data_path, version, log, func, deploy, backend, train_split, config\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-13 16:37:58\u001b[0m | \u001b[36mautotrain.preprocessor.dreambooth\u001b[0m:\u001b[36m_save_concept_images\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mSaving concept images\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-13 16:37:58\u001b[0m | \u001b[36mautotrain.preprocessor.dreambooth\u001b[0m:\u001b[36m_save_concept_images\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mphotos/image (1).jpeg\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-13 16:37:58\u001b[0m | \u001b[36mautotrain.preprocessor.dreambooth\u001b[0m:\u001b[36m_save_concept_images\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mSaving concept images\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-13 16:37:58\u001b[0m | \u001b[36mautotrain.preprocessor.dreambooth\u001b[0m:\u001b[36m_save_concept_images\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mphotos/image (2).jpeg\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-13 16:37:58\u001b[0m | \u001b[36mautotrain.preprocessor.dreambooth\u001b[0m:\u001b[36m_save_concept_images\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mSaving concept images\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-13 16:37:58\u001b[0m | \u001b[36mautotrain.preprocessor.dreambooth\u001b[0m:\u001b[36m_save_concept_images\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mphotos/image.jpeg\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-13 16:37:58\u001b[0m | \u001b[36mautotrain.preprocessor.dreambooth\u001b[0m:\u001b[36m_save_concept_images\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mSaving concept images\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-13 16:37:58\u001b[0m | \u001b[36mautotrain.preprocessor.dreambooth\u001b[0m:\u001b[36m_save_concept_images\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mphotos/image (3).jpeg\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-13 16:37:58\u001b[0m | \u001b[36mautotrain.backends.local\u001b[0m:\u001b[36mcreate\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mStarting local training...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-13 16:37:58\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1m['python', '-m', 'autotrain.trainers.dreambooth', '--training_config', 'my-dreambooth-project/training_params.json']\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-13 16:37:58\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m361\u001b[0m - \u001b[1m{'model': 'stabilityai/stable-diffusion-xl-base-1.0', 'vae_model': None, 'revision': None, 'tokenizer': None, 'image_path': 'my-dreambooth-project/autotrain-data', 'class_image_path': None, 'prompt': 'photo a woman of white complexion, with a curvaceous bursty walking on the street, highlight hair, rim lighting, studio lighting, looking at the camera, up close, perfect eyes, bright colour\" \\\\', 'class_prompt': None, 'num_class_images': 100, 'class_labels_conditioning': None, 'prior_preservation': False, 'prior_loss_weight': 1.0, 'project_name': 'my-dreambooth-project', 'seed': 42, 'resolution': 1024, 'center_crop': False, 'train_text_encoder': False, 'batch_size': 1, 'sample_batch_size': 4, 'epochs': 1, 'num_steps': 500, 'checkpointing_steps': 100000, 'resume_from_checkpoint': None, 'gradient_accumulation': 4, 'disable_gradient_checkpointing': False, 'lr': 0.0001, 'scale_lr': False, 'scheduler': 'constant', 'warmup_steps': 0, 'num_cycles': 1, 'lr_power': 1.0, 'dataloader_num_workers': 0, 'use_8bit_adam': False, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'allow_tf32': False, 'prior_generation_precision': None, 'local_rank': -1, 'xformers': False, 'pre_compute_text_embeddings': False, 'tokenizer_max_length': None, 'text_encoder_use_attention_mask': False, 'rank': 4, 'xl': True, 'mixed_precision': 'fp16', 'token': '*****', 'push_to_hub': True, 'username': 'deepshikhasplore', 'validation_prompt': None, 'num_validation_images': 4, 'validation_epochs': 50, 'checkpoints_total_limit': None, 'validation_images': None, 'logging': False}\u001b[0m\n",
      "\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2024-05-13 16:38:01\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m39\u001b[0m - \u001b[33m\u001b[1mFailed to download dataset: 404 Client Error. (Request ID: Root=1-664241e9-542ac0b908b31ded2e578e98;d8ca5721-5e15-4e3b-b842-6af4df16348b)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/api/datasets/my-dreambooth-project/autotrain-data/revision/main.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated.\u001b[0m\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'thresholding', 'variance_type', 'dynamic_thresholding_ratio', 'rescale_betas_zero_snr', 'clip_sample_range'} was not found in config. Values will be initialized to default values.\n",
      "text_encoder/model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 492M/492M [00:01<00:00, 438MB/s]\n",
      "text_encoder_2/model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.78G/2.78G [00:06<00:00, 450MB/s]\n",
      "vae/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 642/642 [00:00<00:00, 5.67MB/s]\n",
      "vae/diffusion_pytorch_model.safetensors: 100%|â–ˆ| 335M/335M [00:00<00:00, 385MB/s\n",
      "{'latents_std', 'latents_mean'} was not found in config. Values will be initialized to default values.\n",
      "unet/diffusion_pytorch_model.safetensors: 100%|â–ˆ| 10.3G/10.3G [00:38<00:00, 267M\n",
      "{'dropout', 'reverse_transformer_layers_per_block', 'attention_type'} was not found in config. Values will be initialized to default values.\n",
      "\u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[32m2024-05-13 16:38:56\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m120\u001b[0m - \u001b[31m\u001b[1mtrain has failed due to an exception: Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/autotrain/trainers/common.py\", line 117, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/autotrain/trainers/dreambooth/__main__.py\", line 122, in train\n",
      "    main(_args)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/autotrain/trainers/dreambooth/train_xl.py\", line 540, in main\n",
      "    unet.to(accelerator.device, dtype=weight_dtype)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1160, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 6 more times]\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 1.06 MiB is free. Process 18861 has 14.00 GiB memory in use. Including non-PyTorch memory, this process has 758.00 MiB memory in use. Of the allocated memory 628.02 MiB is allocated by PyTorch, and 29.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\u001b[0m\n",
      "\u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[32m2024-05-13 16:38:56\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m121\u001b[0m - \u001b[31m\u001b[1mCUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 1.06 MiB is free. Process 18861 has 14.00 GiB memory in use. Including non-PyTorch memory, this process has 758.00 MiB memory in use. Of the allocated memory 628.02 MiB is allocated by PyTorch, and 29.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-13 16:38:57\u001b[0m | \u001b[36mautotrain.cli.run_dreambooth\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m393\u001b[0m - \u001b[1mJob ID: 8108\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!autotrain dreambooth \\\n",
    "--model ${MODEL_NAME} \\\n",
    "--project-name ${PROJECT_NAME} \\\n",
    "--image-path photos/ \\\n",
    "--prompt \"${PROMPT}\" \\\n",
    "--resolution ${RESOLUTION} \\\n",
    "--batch-size ${BATCH_SIZE} \\\n",
    "--num-steps ${NUM_STEPS} \\\n",
    "--gradient-accumulation ${GRADIENT_ACCUMULATION} \\\n",
    "--lr ${LEARNING_RATE} \\\n",
    "--mixed-precision ${MIXED_PRECISION} \\\n",
    "--username ${HF_USERNAME} \\\n",
    "$( [[ \"$USE_XFORMERS\" == \"True\" ]] && echo \"--xformers\" ) \\\n",
    "$( [[ \"$TRAIN_TEXT_ENCODER\" == \"True\" ]] && echo \"--train-text-encoder\" ) \\\n",
    "$( [[ \"$USE_8BIT_ADAM\" == \"True\" ]] && echo \"--use-8bit-adam\" ) \\\n",
    "$( [[ \"$DISABLE_GRADIENT_CHECKPOINTING\" == \"True\" ]] && echo \"--disable_gradient-checkpointing\" ) \\\n",
    "$( [[ \"$PUSH_TO_HUB\" == \"True\" ]] && echo \"--push-to-hub --token ${HF_TOKEN}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LvIS7-7PcLT"
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "# this is the inference code that you can use after you have trained your model\n",
    "# Unhide code below and change prj_path to your repo or local path (e.g. my_dreambooth_project)\n",
    "#\n",
    "#\n",
    "#\n",
    "from diffusers import DiffusionPipeline, StableDiffusionXLImg2ImgPipeline\n",
    "import torch\n",
    "\n",
    "prj_path = \"username/repo_name\"\n",
    "model = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "     model,\n",
    "     torch_dtype=torch.float16,\n",
    " )\n",
    "pipe.to(\"cuda\")\n",
    "pipe.load_lora_weights(prj_path, weight_name=\"pytorch_lora_weights.safetensors\")\n",
    "\n",
    "refiner = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "     \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "     torch_dtype=torch.float16,\n",
    ")\n",
    "refiner.to(\"cuda\")\n",
    "\n",
    "prompt = \"photo a woman of white complexion, with a curvaceous bursty walking on the street, highlight hair, rim lighting, studio lighting, looking at the camera, up close, perfect eyes, bright colour\\\" \\\\\" # @param {type: \"string\"}\n",
    "\n",
    "\n",
    "# seed = 42\n",
    "# generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
    "# image = pipe(prompt=prompt, generator=generator).images[0]\n",
    "# image = refiner(prompt=prompt, generator=generator, image=image).images[0]\n",
    "# image.save(f\"generated_image.png\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
